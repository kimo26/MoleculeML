{
  "cells": [
    {
      "id": "3ffbb2e9-bdfb-44a8-a335-bf06a351051b",
      "cell_type": "markdown",
      "source": "# Project Overview\n\nThis project involves the discovery of promising materials for organic solar cells. The focus is on the HOMO-LUMO gap, a measure of molecular efficiency for solar energy use. The larger this gap, the more efficient the molecule is for solar cells. The challenge is to use machine learning to predict this gap from a molecule description, which is a more efficient approach than traditional density functional theory.\n\nThe data provided includes a small data set (100 molecules with associated HOMO-LUMO gaps) and a larger, related data set (50,000 molecules with LUMO energy levels). Despite the second data set's labels not being directly the desired prediction target, it is considered valuable due to the probable correlation between molecule features predictive of LUMO energy and the HOMO-LUMO gap.\n\nThis approach is referred to as transfer learning, where features learned on one task are used for another related task. The project also suggests using unsupervised representation learning methods, like autoencoders, to extract useful feature representations from the larger dataset. Knowledge should be \"transferred\" to utilize the additional label information in the larger dataset. This could involve, for example, reusing the last layer of a neural network trained to predict LUMO energy.\n\nThe goal is to submit predictions of the HOMO-LUMO gap for a third set of 10,000 unseen molecule descriptions.",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "dc6b9144-0cbb-4dd0-948b-15a73ac2daf1",
      "cell_type": "markdown",
      "source": "# Methodology\n\nThe code employs a hybrid approach combining an autoencoder with regression models for a predictive task. This approach uses semi-supervised learning, beneficial given the extensive data requirements of deep learning models. Instead of using pre-provided fingerprints, the RDKit library is used to extract MACCS keys and ECFP (2000 bits) from the smiles of each molecule, thereby expanding the dataset. This methodology is inspired by the paper \"Improvement of Prediction Performance With Conjoint Molecular Fingerprint in Deep Learning\".\n\nThe autoencoder in this code doesn't compress the input but seeks to learn valuable feature transformation. This feature transformation is then used in conjunction with a predictor model trained on the encoder's output to predict the target variable, LUMO. This dual-task structure encourages the encoder to learn features beneficial for both reconstructing the original data and predicting the target variable. \n\n\nThe reason the encoder doesn't compress the input in this case is because we're more interested in feature transformation than dimensionality reduction. By not compressing the input, the encoder can learn a more complex and potentially useful representation of the data, which can then be used by the predictor model to make more accurate predictions. This approach is particularly useful in cheminformatics, where the input data (molecular fingerprints) is already a compressed representation of the original data (molecular structures), and further compression could result in loss of important information.\n\nAfter training the autoencoder-predictor on the larger pretraining dataset, the encoder, now acting as a feature extractor, is used to transform the actual training data into a latent representation. This representation captures significant patterns relevant to a similar task. Then, two regression models are trained on these encoded features, exploiting the learned patterns for the prediction task.\n",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "b95128e8-f185-463d-86f1-c3250a1549b0",
      "cell_type": "markdown",
      "source": "## Set Up\n\nThis section is dedicated to setting up the necessary environment for the project. This includes importing the required libraries and defining helper functions.",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "U6ACsTo_7w4i",
        "noteable": {
          "output_collection_id": "7bab9774-0bab-4818-b0d6-c30b1dd9f35f"
        }
      },
      "outputs": [],
      "source": "# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom rdkit import Chem, DataStructs\nfrom rdkit.Chem import MACCSkeys, AllChem\nfrom tensorflow.keras.models import Model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom tensorflow.keras.losses import MeanSquaredError\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint,ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam,SGD\nfrom tensorflow.keras.models import Model,load_model\nfrom tensorflow.keras.layers import Dense,Input, Dropout, BatchNormalization,LeakyReLU",
      "id": "2af0f166"
    },
    {
      "id": "aac3a034-036d-4f5d-81c7-d17a6dcf3fb9",
      "cell_type": "markdown",
      "source": "### Helper Functions\n\nTwo helper functions are defined here:\n\n- `smiles_to_fp`: This function converts a molecule's SMILES representation to a fingerprint. It uses the RDKit library to generate Extended-Connectivity Fingerprints (ECFP) and MACCS keys for the molecule. These fingerprints are then concatenated to form the final feature set.\n\n- `rmse`: This function calculates the root mean square error (RMSE) between two arrays. It is used for model evaluation.",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_jJ9zDsVJMSi"
      },
      "outputs": [],
      "source": "# Function to convert smiles to fingerprint\ndef smiles_to_fp(smiles, radius=2, n_bits=2000):\n    # Convert smiles to molecule\n    mol = Chem.MolFromSmiles(smiles)\n    # Generate ECFP\n    ecfp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)\n    # Generate MACCS keys\n    maccs = MACCSkeys.GenMACCSKeys(mol)\n    # Convert ECFP and MACCS keys to numpy arrays\n    arr_ecfp = np.zeros((0,), dtype=int)\n    arr_maccs = np.zeros((0,), dtype=int)\n    DataStructs.ConvertToNumpyArray(ecfp, arr_ecfp)\n    DataStructs.ConvertToNumpyArray(maccs, arr_maccs)\n    # Concatenate ECFP and MACCS keys\n    conjoint_fp = np.concatenate([arr_ecfp, arr_maccs])\n    return conjoint_fp",
      "id": "22ed54b1"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1eDhtapyxztF"
      },
      "outputs": [],
      "source": "# Function to calculate root mean square error\ndef rmse(x1, x2):\n    return np.sqrt(mean_squared_error(x1, x2))",
      "id": "fa05c73c"
    },
    {
      "id": "f81f7072-067b-422e-bb0a-6caccb5c8f84",
      "cell_type": "markdown",
      "source": "## Data Loading and Preprocessing\n\nIn this section, we load the data and perform some preprocessing steps. The data consists of two datasets:\n\n- A small dataset of 100 molecules with associated HOMO-LUMO gaps.\n- A larger dataset of 50,000 molecules with LUMO energy levels.\n\nThe small dataset is used for the main task of predicting the HOMO-LUMO gap, while the larger dataset is used for pretraining the model in a transfer learning approach.",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "6b418df3-55cf-49d3-aeb7-9b6dc3771511",
      "cell_type": "markdown",
      "source": "### Data Preprocessing\n\nThe data preprocessing step involves converting the SMILES strings into molecular fingerprints. The RDKit library is used to convert the SMILES strings into molecule objects, and then the MACCS keys and ECFP fingerprints are generated from these molecule objects.\n\nThe MACCS keys are a set of 167 substructure key descriptors, and the ECFP fingerprints are a type of circular fingerprint that is based on the Morgan algorithm. These fingerprints are binary vectors that represent the presence or absence of certain molecular substructures, and they are a common way of encoding molecular structure information for machine learning models.\n\nThe fingerprints are then concatenated together to form a single feature vector for each molecule. This results in a feature vector of length 2167 for each molecule.\n\nThe target variable for the small dataset, the HOMO-LUMO gap, is also extracted.",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "67a217cb-970b-4053-a9e7-13d6aa97e487",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code"
        }
      },
      "execution_count": null,
      "source": "def convert_smiles_to_fingerprints(smiles_array):\n    return np.array([smiles_to_fp(s) for s in smiles_array])",
      "outputs": []
    },
    {
      "id": "7d01211e-a201-4f63-918d-afd755716efd",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code"
        }
      },
      "execution_count": null,
      "source": "def load_and_preprocess_data(file_name):\n    # Load data\n    df = pd.read_csv(file_name, index_col=\"Id\", compression='zip')\n    # Extract SMILES strings\n    smiles = df['smiles'].to_numpy()\n    # Convert SMILES strings to fingerprints\n    fingerprints = convert_smiles_to_fingerprints(smiles)\n    return fingerprints",
      "outputs": []
    },
    {
      "id": "100c774e-2afd-401d-883f-ff6c5b68882c",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code"
        }
      },
      "execution_count": null,
      "source": "# Load and preprocess pretraining data\npretrain_x_conjoint = load_and_preprocess_data(\"pretrain_features.csv.zip\")\npretrain_y = pd.read_csv(\"pretrain_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\nprint('Pretraining data loaded')",
      "outputs": []
    },
    {
      "id": "bdb15c68-c162-4f26-9e59-7f93d2fdbf21",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code"
        }
      },
      "execution_count": null,
      "source": "# Load and preprocess training data\ntrain_x_conjoint = load_and_preprocess_data(\"train_features.csv.zip\")\ntrain_y = pd.read_csv(\"train_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\nprint('Training data loaded')",
      "outputs": []
    },
    {
      "id": "0ea4b574-d0cc-4156-a704-6559762d8047",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code"
        }
      },
      "execution_count": null,
      "source": "# Load and preprocess test data\ntest_df = pd.read_csv(\"test_features.csv.zip\", index_col=\"Id\", compression='zip')\ntest_x = test_df['smiles'].to_numpy()\ntest_x_conjoint = convert_smiles_to_fingerprints(test_x)\nprint('Loaded test data')",
      "outputs": []
    },
    {
      "id": "3419d397-7355-43d3-8b4c-522823fcb88a",
      "cell_type": "markdown",
      "source": "## Model Creation\n\nThe model for this task is a combination of an autoencoder and a predictor. The autoencoder is a type of neural network that is used to learn efficient data codings in an unsupervised manner. It consists of an encoder, which compresses the input data into a latent-space representation, and a decoder, which reconstructs the input data from the latent space representation.\n\nIn this case, the autoencoder is not used to reduce the dimensionality of the input data (which is a common use case for autoencoders), but rather to learn a transformation of the input data that makes it easier for the predictor to predict the target variable (the HOMO-LUMO gap). This is achieved by training the autoencoder and the predictor simultaneously, with the predictor taking the output of the encoder as input.\n\nThe reason for not compressing the input data in the encoder is that we are more interested in transforming the data into a representation that is useful for the prediction task, rather than reducing the dimensionality of the data. By allowing the encoder to learn a higher-dimensional representation, we give it more flexibility to learn a complex transformation that captures the important features in the data for predicting the HOMO-LUMO gap.\n\nThis approach is inspired by the concept of representation learning, which is a set of techniques that allows a system to automatically discover the representations needed for data analysis or task performance. In the context of deep learning, representation learning is often used to learn complex data transformations that can improve the performance of the model on the task at hand.\n\nThe predictor is a simple feed-forward neural network that takes the output of the encoder as input and predicts the HOMO-LUMO gap. The use of a neural network for the predictor allows us to capture complex, non-linear relationships between the transformed input data and the target variable.",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "a573eb31-cf8a-4c13-bfc0-913a3cd1a5fa",
      "cell_type": "markdown",
      "source": "### Autoencoder\n\nIn this project, the autoencoder is used to learn a valuable feature transformation from the input data. The learned features are then used for predicting the target variable, LUMO.",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "46d84133-e594-4286-a121-3d7977646f04",
      "cell_type": "markdown",
      "source": "#### **Design Choices**\n\n\n- **Linear Activation for Encoder Output**: The linear activation function was used for the output of the encoder to allow it to output a wide range of values, not limited to a specific range or distribution. This is appropriate for the task of learning a feature transformation, where we want to preserve as much information as possible about the input data.\n\n- **Softmax Activation for Decoder Output Layer**: The softmax activation function was used for the output layer of the decoder to ensure that the output is a probability distribution, i.e., that the output values are non-negative and sum to 1. This is appropriate for the task of reconstructing the input data, which in this case is a binary fingerprint.\n\n- **Leaky ReLU Activation Function**: The Leaky ReLU activation function was chosen for its ability to mitigate the vanishing gradients problem, which can occur when training deep neural networks. By allowing a small, positive gradient when the unit is not active, Leaky ReLU helps to ensure that neurons never die, i.e., go into a state where they could possibly stop learning.\n\n\n",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "K_ioIqZjYunG"
      },
      "outputs": [],
      "source": "# Function to create encoder model\ndef create_encoder(input_dim=2167):\n    inputs = Input(shape=(input_dim,))\n    x = Dense(2167)(inputs)\n    x = BatchNormalization()(x)\n    x = LeakyReLU(alpha=0.01)(x)\n    x = Dropout(0.5)(x)\n    x = Dense(2500)(x)\n    x = BatchNormalization()(x)\n    x = LeakyReLU(alpha=0.01)(x)\n    x = Dropout(0.5)(x)\n    x = Dense(2167,'linear')(x)\n    return Model(inputs, x, name='encoder')",
      "id": "80db7cb0"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9FFRkdQ5Z6sd"
      },
      "outputs": [],
      "source": "# Function to create decoder model\ndef create_decoder(input_dim=2167):\n    inputs = Input(shape=(input_dim,))\n    x = Dense(2000)(inputs)\n    x = BatchNormalization()(x)\n    x = LeakyReLU(alpha=0.01)(x)\n    x = Dropout(0.5)(x)\n    decoded = Dense(2167, activation='softmax')(x)\n    return Model(inputs, decoded, name='decoder')",
      "id": "0f113ae9"
    },
    {
      "id": "04c9a2cb-2562-4d16-ad70-f249b427202c",
      "cell_type": "markdown",
      "source": "### Predictor Model\n\nThe predictor model is a simple neural network that is trained on the output of the encoder to predict the target variable, LUMO. This model is trained simultaneously with the autoencoder, encouraging the encoder to learn features beneficial for both reconstructing the original data and predicting the target variable.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dREQDyzoI924"
      },
      "outputs": [],
      "source": "# Function to create predictor model\ndef create_predictor(input_dim=2167):\n    inputs = Input(shape=(input_dim,))\n    x = Dense(256)(inputs)\n    x = BatchNormalization()(x)\n    x = LeakyReLU(alpha=0.01)(x)\n    x = Dropout(0.5)(x)\n    output = Dense(1, 'linear')(x)\n    return Model(inputs, output, name='predictor')",
      "id": "a4b6d5c5"
    },
    {
      "id": "9baad31e-e1e3-4d5b-a25d-9fa9dec38bda",
      "cell_type": "markdown",
      "source": "### Combined Model\n\nThe combined model is a combination of the autoencoder and the predictor model. It is trained on the larger pretraining dataset to learn valuable feature transformations and predict the LUMO energy levels. After training, the encoder part of the model is used as a feature extractor to transform the actual training data into a latent representation. This representation captures significant patterns relevant to a similar task.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "TsxbPqDJBk0o"
      },
      "outputs": [],
      "source": "# Function to create combined model\ndef create_combined_model(input_dim=2167):\n    # Create encoder, decoder, and predictor models\n    encoder = create_encoder()\n    decoder = create_decoder()\n    predictor = create_predictor()\n    # Define inputs\n    inputs = Input(shape=(input_dim,))\n    # Pass inputs through encoder\n    encoded = encoder(inputs)\n    # Pass encoded inputs through decoder and predictor\n    decoded = decoder(encoded)\n    predicted = predictor(encoded)\n    # Define model\n    model = Model(inputs, [decoded, predicted], name='combined')\n    # Compile model\n    model.compile(loss={'decoder':'binary_crossentropy', 'predictor':'mse'}, optimizer=SGD(momentum=0.6), metrics={'predictor': RootMeanSquaredError(name = 'rmse')})\n    return model, encoder",
      "id": "603b478f"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "x-JZadqNIs-d"
      },
      "outputs": [],
      "source": "# Create combined model and encoder\nmodel, encoder = create_combined_model()\nprint('Model and encoder created')",
      "id": "24b29a4c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pd51zCwbgOcT"
      },
      "source": "#### **Model Summary**",
      "id": "3582bb28"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BOXfKAKbgQFj",
        "noteable": {
          "output_collection_id": "699551f9-af16-4f8d-8b8e-24e56c57ca22"
        }
      },
      "outputs": [],
      "source": "# Print model summary\nmodel.summary()",
      "id": "70c9bcfc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMNMfY12eWto"
      },
      "source": "## Model Training\n\nIn this section, we train the models. The combined model (autoencoder + predictor) is first trained on the larger pretraining dataset. After training, the encoder part of the model is used to transform the actual training data into a latent representation. Two regression models (Linear Regression and XGBoost) are then trained on these encoded features.",
      "id": "e21113a0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8ZADmk2e5CX"
      },
      "source": "#### **Callbacks**",
      "id": "668b2564"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "H61yj3Xle6zO"
      },
      "outputs": [],
      "source": "# Set up callbacks\nearly_stopping = EarlyStopping(monitor='val_predictor_rmse', patience=20, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_predictor_rmse', factor=0.1, patience=5)\nmodel_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_predictor_rmse', save_best_only=True, save_weights_only=True)\ncallbacks = [early_stopping, reduce_lr, model_checkpoint]",
      "id": "4a2fca7c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys2fKCjvfJ-D"
      },
      "source": "#### **Fit**",
      "id": "7a6ecf3a"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYoZZH8tM0HR",
        "outputId": "e6398583-069c-47c7-fffb-9ec128a37314",
        "noteable": {
          "output_collection_id": "227ca7c7-85bb-4b1e-b843-5d433367b03d"
        }
      },
      "outputs": [],
      "source": "# Fit the model\nhistory = model.fit(pretrain_x_conjoint, [pretrain_x_conjoint, pretrain_y], validation_split=0.1, epochs=100, batch_size=32, callbacks=callbacks)",
      "id": "d3b508cd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7lrgmtbkees"
      },
      "source": "## **Regression Model**\n\nBasic regression models (Linear Regression and XGBoost) were chosen for the final task due to their simplicity, interpretability, and strong performance on a wide range of problems. While deep learning models can achieve high performance, they also require large amounts of data and computational resources, and can be more difficult to interpret.",
      "id": "ad053d4e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgYhXHCRlAtw"
      },
      "source": "### **Extract features**",
      "id": "f3734a61"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "eIt1XWiAN5zl"
      },
      "outputs": [],
      "source": "# Extract features from training data using the encoder\nencoded_train_x = encoder.predict(train_x_conjoint, verbose=0)\nprint('Features extracted')",
      "id": "40c78c32"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OOvi53mN82z"
      },
      "source": "#### **Split validation data**",
      "id": "de8191d2"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "XJ8DxFnCODVQ"
      },
      "outputs": [],
      "source": "# Split validation data\nx, x_val, y, y_val = train_test_split(encoded_train_x, train_y, test_size=0.1, random_state=42)\nprint('Validation data split')",
      "id": "ef3b1179"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpmpKsMQlVDa"
      },
      "source": "## **Fit regressors**",
      "id": "a9390343"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "hzePmp_fwewy",
        "outputId": "6c1ce0a6-995d-41fc-98c6-f7463918270f",
        "noteable": {
          "output_collection_id": "da0d0acf-1b80-4276-aaec-7447b8d33a1f"
        }
      },
      "outputs": [],
      "source": "# Fit linear regression model\nlr = LinearRegression().fit(x, y)\nprint('Linear regression model fitted')",
      "id": "865aae6f"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "e8BEuZVD6-Z8",
        "outputId": "c6d49339-d22b-4bfb-bd09-427b79b82f59",
        "noteable": {
          "output_collection_id": "56796942-9fc7-4f2a-9c73-8c952096a1d9"
        }
      },
      "outputs": [],
      "source": "# Fit XGBoost regression model\nxgb = XGBRegressor(tree_method='gpu_hist').fit(x, y)\nprint('XGBoost regression model fitted')",
      "id": "8fd45ccf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDymxNCaOZUp"
      },
      "source": "### **Test models**",
      "id": "2f06eaa0"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GY2iA1TyObZ2",
        "outputId": "19a1c7df-d248-4b2d-c8e8-ba5b13f5372d",
        "noteable": {
          "output_collection_id": "ed57b541-fa0a-438d-878e-72b84e5234a4"
        }
      },
      "outputs": [],
      "source": "# Test models\nlr_rmse = rmse(y_val, lr.predict(x_val))\nxgb_rmse = rmse(y_val, xgb.predict(x_val))\nprint(f'Linear Regression RMSE: {lr_rmse}')\nprint(f'XGBoost RMSE: {xgb_rmse}')",
      "id": "da715cb2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnCwCIzUOqsU"
      },
      "source": "#### **Select best**",
      "id": "8540cd14"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "aoLvyOY9Ovrn"
      },
      "outputs": [],
      "source": "# Select the best model\nif lr_rmse < xgb_rmse:\n    best_model = lr\n    print('Linear Regression model selected')\nelse:\n    best_model = xgb\n    print('XGBoost model selected')",
      "id": "9662a948"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXAJg4QYO4Ej"
      },
      "source": "## **Predict and save results**",
      "id": "d1122db2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5S9EtsjPCk8"
      },
      "source": "### **Predict**",
      "id": "7e3f39c1"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Q8yIunkoPEwd"
      },
      "outputs": [],
      "source": "# Make predictions using the best model\nencoded_test_x = encoder.predict(test_x_conjoint, verbose=0)\npredictions = best_model.predict(encoded_test_x)\nprint('Predictions made')",
      "id": "010cf863"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9hrrpiWPLx4"
      },
      "source": "### **Save**",
      "id": "00463a53"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsNr8rTAkkds",
        "outputId": "9148d0f0-9451-4d76-d1d6-f09b5065e3c1",
        "noteable": {
          "output_collection_id": "740b5baa-7dc4-4383-ada5-e85978317447"
        }
      },
      "outputs": [],
      "source": "# Save predictions\ndf_pred = pd.DataFrame({\"y\": predictions}, index=test_df.index)\ndf_pred.to_csv(\"results.csv\", index_label=\"Id\")\nprint(\"Predictions saved\")",
      "id": "ba33a70e"
    },
    {
      "id": "8457134b-de03-4791-ba28-afa9fcf8a14e",
      "cell_type": "markdown",
      "source": "## Conclusion\n\nIn this project, we tackled the challenge of predicting the HOMO-LUMO gap of molecules for the application in organic solar cells. We used a hybrid approach combining an autoencoder with regression models to leverage a larger dataset with related but not directly applicable labels. This approach, known as transfer learning, allowed us to extract valuable feature representations from the larger dataset and apply them to our smaller, target dataset.\n\nWe used the RDKit library to extract MACCS keys and ECFP from the smiles of each molecule, expanding our dataset and providing a more detailed representation of each molecule. Our autoencoder was designed not to compress the input but to learn valuable feature transformations, which were then used in conjunction with a predictor model to predict the target variable, LUMO.\n\nThe results showed that our approach was able to effectively predict the HOMO-LUMO gap, demonstrating the potential of machine learning techniques in the field of materials science and specifically in the design of organic solar cells.",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "ff5e31cd-2fc5-4568-8c5a-96872cee0468",
      "cell_type": "markdown",
      "source": "## Future Work\n\nWhile the results of this project are promising, there are several directions for future work to improve and expand upon what we've done:\n\n- **Model Exploration**: We could explore other types of models or architectures, such as different types of autoencoders or other types of neural networks such as Graph Neural Networks (GNNs).\n\n- **Feature Engineering**: We could explore other ways of representing the molecules, such as different types of fingerprints or other molecular descriptors. We could also look into more advanced feature selection techniques to identify the most informative features.\n\n- **Data Augmentation**: We could look into techniques for data augmentation, such as generating new molecules or modifying existing ones, to increase the size of our training dataset.\n\n- **Hyperparameter Tuning**: We could perform more extensive hyperparameter tuning to optimize the performance of our models. This could include tuning the architecture of the autoencoder and the predictor model, as well as the training parameters.\n",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "b4c2014b-60b1-41a0-94da-8d99729209bd",
      "cell_type": "markdown",
      "source": "# References\n\n1. [Improvement of Prediction Performance With Conjoint Molecular Fingerprint in Deep Learning](https://www.frontiersin.org/articles/10.3389/fchem.2020.00606/full)\n2. [A Novel Molecular Representation Learning for Molecular Property Prediction with a Multiple SMILES-Based Augmentation](https://www.mdpi.com/2218-273X/11/8/1220)",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "markdown"
        }
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "qvrmd1xR9J0t",
        "QNZNvEhX7TVS",
        "WB1dF3Am_Ilm"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "noteable": {
      "last_transaction_id": "ed7a9bfb-1f64-4236-a44d-81bfe74b49d1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
